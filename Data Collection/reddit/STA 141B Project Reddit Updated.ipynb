{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import json\n",
    "import datetime \n",
    "import csv\n",
    "import praw\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#got it from reddit\n",
    "CLIENT_ID = \"\" \n",
    "SECRET_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY) #getting authorizations\n",
    "auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting password in txt file instead of writing it down\n",
    "f = open(\"password_reddit.txt\",\"r\")\n",
    "lines=f.readlines()\n",
    "password=lines[0]\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit API needs this in this format\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'randomlymixed',\n",
    "    'password': password\n",
    "}\n",
    "user_agent = \"MyAPI 1.0\"\n",
    "headers = {'User-Agent': \"MyAPI/0.0.1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit API \n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token', \n",
    "                    auth=auth, data= data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the token to access reddit api\n",
    "TOKEN = res.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers['Authorization'] = f'bearer {TOKEN}'\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1329358479ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://oauth.reddit.com/api/v1/me'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "#testing if I have access with my credentials \n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers = headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for praw, another reddit scraper. I did not use it for this project but this is how I coded it \n",
    "reddit = praw.Reddit(\n",
    "    client_id = CLIENT_ID,\n",
    "    client_secret = SECRET_KEY,\n",
    "    user_agent = user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-dd65bdf98a28>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-dd65bdf98a28>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    url = 'https://api.pushshift.io/reddit/search/comment/?q='+\u001b[0m\n\u001b[1;37m                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_reddit_data(after, before, term, sub, limit): #pushshift is the scraper I used, more info at https://github.com/pushshift/api\n",
    "    ''' Purpose of the function is to get all reddit comments and additional with certain other informationkey terms \n",
    "    within the period of after and before days. All the reddit information is stored in a dictionary.\n",
    "    after: is a string in the format of \"#d\". Ex: 720d\n",
    "    before: is a string in the format of \"#d\". Ex: 500d\n",
    "    term: key words that we will search in. String format. if spaces adds dashes instead. Ex: \"Covid-Vaccine\"\n",
    "    sub: is a string in the format of \"sub\" Short for subreddit. Ex: \"AskReddit\"\n",
    "    limit: how many comments should it pull. Max limit is 100. in a str format \"#\". Ex: \"100\"\n",
    "    '''\n",
    "    url = 'https://api.pushshift.io/reddit/search/comment/?q='+ str(term) + '&after=' +str(after)+ '&before='+str(before)+'&subreddit=' +str(sub) +'&size='+str(limit)\n",
    "    print(url) #gives us the url so we can go to the site ourselves to see what is on it \n",
    "    res = requests.get(url) #running to get what we need \n",
    "    data = json.loads(res.text, strict = False) #puts it in a dictionary for us \n",
    "    return data['data'] #returning the important thing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.DataFrame() #initalizing a dataframe outside so I can have a copy of it\n",
    "def comment_reddit(after, before, term, subreddit, limit):\n",
    "    '''\n",
    "    Purpose is to store all the comments in a dataframe that we scraped from the other function. Return it in a pd.DataFrame()\n",
    "    after: is a string in the format of \"#d\". Ex: 720d\n",
    "    before: is a string in the format of \"#d\". Ex: 500d\n",
    "    after: is a string in the format of \"#d\". Ex: 720d\n",
    "    before: is a string in the format of \"#d\". Ex: 500d\n",
    "    term: key words that we will search in. String format. if spaces adds dashes instead. Ex: \"Covid-Vaccine\"\n",
    "    sub: is a string in the format of \"sub\" Short for subreddit. Ex: \"AskReddit\"\n",
    "    limit: how many comments should it pull. Max limit is 100. in a str format \"#\". Ex: \"100\"\n",
    "    '''\n",
    "    data = get_reddit_data(str(after), str(before), str(term), str(subreddit), str(limit)) #using other function \n",
    "    for submission in data: \n",
    "        df_comments = df_comments.append({'subreddit': submission['subreddit'],\n",
    "                    'score': submission['score'],\n",
    "                    'comments': submission['body'],\n",
    "                    'time_created': datetime.utcfromtimestamp(submission['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                   }, ignore_index = True) #creates our df \n",
    "    #name_csv = str(input(\"Name of CSV: \"))\n",
    "    #df.to_csv(name_csv+\".csv\", index=False)\n",
    "    return(df_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"vaccine-hate\",\"anti-vaccine\", \"COVID-vaccine\"] list that was used \n",
    "#the purpose was to create all the dfs we created into csv to store \n",
    "def data_collection_reddit(list_name, subreddit):\n",
    "    '''\n",
    "    The purpose was to create all the dfs we created into csv to store. Based on some requirements\n",
    "    list_name: a list of key terms that should be searched on. List should be strings. Ex: [\"vaccine-hate\",\"anti-vaccine\", \"COVID-vaccine\"]\n",
    "    subreddit: what subreddit we want to search the terms on. In str format. Ex: \"Republican\"\n",
    "    '''\n",
    "    counter = 1 #to name the file easier \n",
    "    for i in list_name: \n",
    "        for j in range(720, 540, -20): #in increments of 20 days \n",
    "            try:\n",
    "                df = comment_reddit(str(j)+\"d\", \"540d\", i, str(subreddit), '400')\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    df.to_csv(\"Data/03_11_2020-09_07_2020, Subreddit-\" + str(subreddit) + str(counter) +\".csv\", index=False)\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "    for i in list_name:\n",
    "        for j in range(540, 360, -20):\n",
    "            try:\n",
    "                df = comment_reddit(str(j)+\"d\", \"360d\", i, str(subreddit), '400')\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    df.to_csv(\"Data/09_11_2020-03_06_2021, Subreddit-\" + str(subreddit) + str(counter) +\".csv\", index=False)\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    for i in list_name:\n",
    "        for j in range(360, 180, -15):\n",
    "            try:\n",
    "                df = comment_reddit(str(j)+\"d\", \"180d\", i, str(subreddit), '400')\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    df.to_csv(\"Data/03_06_2021-09_02_2021, Subreddit-\" + str(subreddit) + str(counter) +\".csv\", index=False)\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "    for i in list_name:\n",
    "        for j in range(180, 0, -9):\n",
    "            try:\n",
    "                df = comment_reddit(str(j)+\"d\", \"0d\", i, str(subreddit), '400')\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                else:\n",
    "                    df.to_csv(\"Data/09_02_2021-03_01_2022, Subreddit-\" + str(subreddit) + str(counter) +\".csv\", index=False)\n",
    "                    counter += 1\n",
    "            except:\n",
    "                continue\n",
    "#even though the size is 400, the max is 100. They didnt update the document for pushshift api for the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allows for importing zip\n",
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"DataReddit.zip\", 'r')\n",
    "files.extractall('C:/Users/Wesley Tat/STA 141B/Data')\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wesle\\\\STA 141B'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_csv_files(dates, subreddit, folder): #find all the csvs and makes it into a list \n",
    "    '''\n",
    "    Find all the csvs and makes it into a list. It is a list within a list \n",
    "    dates: list format. It is which dates you have collected your data, essentially naming scheme. \n",
    "    subreddit: Which subreddit, naming scheme for your data. Ex: \"Republican\"\n",
    "    folder: where are the CSVs located? Based on your working directory + folder. Str format\n",
    "    '''\n",
    "    csv_list = []\n",
    "    for i in dates:\n",
    "        csv_list.append(glob.glob(os.path.join(os.getcwd()+\"\\\\\"+str(folder), str(i) +\", Subreddit-\" + str(subreddit)+ \"*.csv\")))\n",
    "    return(csv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {} #initalizing dataframe so I can have a copy\n",
    "def conv_csv_to_pd(sub): \n",
    "    '''\n",
    "    Purpose is to collect all the csvs we found then convert it back to dataframes and combine them with one another based on\n",
    "    the subreddit we chose.\n",
    "    sub: list format, should be the lsit of subreddit used. Ex: [\"Republican\", \"Democrats\",\"Conservatives\"]\n",
    "    '''\n",
    "    for name in sub:\n",
    "        dataset = all_csv_files([\"03_11_2020-09_07_2020\",\"09_11_2020-03_06_2021\",\"03_06_2021-09_02_2021\",\"09_02_2021-03_01_2022\",], str(name), \"Data\")\n",
    "        try: #in case of failure \n",
    "            for i in range(0,4):\n",
    "                data_ = pd.concat(map(pd.read_csv, dataset[i]), ignore_index=True) #adds all the dataframes together \n",
    "            if name in df.keys(): #if it exists, no need and just add dataframe to df \n",
    "                df[name] = data_\n",
    "            else: #making a new dataframe each time for the new subreddit\n",
    "                df[name] = pd.DataFrame()\n",
    "                df[name] = data_\n",
    "        except:\n",
    "            print(\"We have no data on that \" + str(name) + \" subreddit.\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['CoronavirusCA',\n",
    " 'Republican',\n",
    " 'AskReddit',\n",
    " 'Conservative',\n",
    " 'Democrats',\n",
    " 'socialism',\n",
    " 'Liberal',\n",
    " 'moderatepolitics',\n",
    " 'UCDavis',\n",
    " 'Libertarian',\n",
    " 'Politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_csv_to_pd(subreddits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
