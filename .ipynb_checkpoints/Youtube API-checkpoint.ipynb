{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "4782d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build \n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller\n",
    "import nltk \n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "6a498017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(vid_id):\n",
    "    # create list for comments \n",
    "    comments=[]\n",
    "    \n",
    "    # create the youtube object to use api\n",
    "    api_key='AIzaSyApsMarj96xas7ekAIgHUoWJQEb54HAt9o'\n",
    "    youtube= build('youtube','v3',developerKey=api_key)\n",
    "    \n",
    "    # obtain video results \n",
    "    \n",
    "    data=youtube.commentThreads().list(\n",
    "    part='snippet,replies',\n",
    "    videoId=vid_id,\n",
    "    searchTerms=\"vaccine\",\n",
    "    maxResults=100).execute()\n",
    "    \n",
    "    num_iterations=1\n",
    "    \n",
    "    while data: \n",
    "        for items in data['items']:\n",
    "            #get comments and add to list \n",
    "            comment=items['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append(comment)\n",
    "            # extract replies (if any) \n",
    "            #num_replies=items['snippet']['totalReplyCount']\n",
    "            #if num_replies>0: \n",
    "                #for replies in items['replies']['comments']:\n",
    "                    #reply=replies['snippet']['textDisplay']\n",
    "                    #comments.append(reply)\n",
    "        if 'nextPageToken' in data and num_iterations <20: \n",
    "            data=youtube.commentThreads().list(\n",
    "            part='snippet,replies',\n",
    "            videoId=vid_id,\n",
    "            searchTerms=\"vaccine\",\n",
    "            pageToken=data['nextPageToken'],\n",
    "            maxResults=100).execute()   \n",
    "            \n",
    "            num_iterations+=1\n",
    "        else:\n",
    "            break \n",
    "            \n",
    "    df=pd.DataFrame(comments,columns=[\"comments\"])\n",
    "    return(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "6be47026",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "#add more words to remove \n",
    "stopwords.extend([\"vaccine\",\"vaccinate\",\"vaccinated\",\"vaccination\",\"covid\",\"vaccines\"])\n",
    "\n",
    "def stop_words_remover(sentence):\n",
    "    \"\"\"Removes stop words from sentences and returns back the filtered sentence as a string\"\"\"\n",
    "    return \" \".join([i for i in nltk.word_tokenize(sentence) if i not in stopwords])\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    \"\"\"Removes any non alphanumeric\"\"\"\n",
    "    return \" \".join([i for i in nltk.word_tokenize(sentence) if i.isalnum()])\n",
    "\n",
    "def to_lower(sentence):\n",
    "    return \" \".join([i.lower() for i in nltk.word_tokenize(sentence)])\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(sentence): \n",
    "    return \" \".join([lemmatizer.lemmatize(i) for i in nltk.word_tokenize(sentence)])\n",
    "\n",
    "corrector=Speller(lang='en')\n",
    "\n",
    "def preprocess(df,col): \n",
    "    return df[col].apply(to_lower).apply(stop_words_remover).apply(remove_punctuations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "2c273bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_blob_sentiment(df,col): \n",
    "    blob_objects=df[col].apply(TextBlob)\n",
    "    sentiment_scores=[{\"polarity\":i.sentiment.polarity,\n",
    "                       \"subjectivity\":i.sentiment.subjectivity\n",
    "                      }for i in blob_objects.values]\n",
    "    sentiment_scores_df=pd.DataFrame(sentiment_scores)\n",
    "    sentiment_df=pd.concat([df,sentiment_scores_df],axis=1)\n",
    "    return(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "12799ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment(df,col):\n",
    "    sia=SentimentIntensityAnalyzer()\n",
    "    sentiment_scores=df[col].apply(sia.polarity_scores)\n",
    "    # append scores to dataframe \n",
    "    df['negative']=sentiment_scores.apply(lambda neg:neg[\"neg\"])\n",
    "    df['neutral']=sentiment_scores.apply(lambda neu:neu[\"neu\"])\n",
    "    df['positive']=sentiment_scores.apply(lambda pos:pos[\"pos\"])\n",
    "    df['compound']=sentiment_scores.apply(lambda comp:comp[\"compound\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "a04618bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(df):\n",
    "    wc=WordCloud(width=800,height=800,background_color='white',\n",
    "                 stopwords=STOPWORDS).generate(\" \".join(df.processed_comments))\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.imshow(wc,interpolation='bilinear')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "eb89acd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
